<html lang="zh">
  <head>
    <meta charset="UTF-8"/>
    <!-- Load TensorFlow.js -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.11.7"></script>
    <!-- Load Posenet -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet@0.1.2"></script>
 </head>

  <body>
    <div id="loading"></div>
    <div id="info"></div>
    <input id="clear" type="button" value="clear"/>
    <div id="main" style="display:none">
      <video id="video" playsinline style=" -moz-transform: scaleX(-1);
          -o-transform: scaleX(-1);
          -webkit-transform: scaleX(-1);
          transform: scaleX(-1);
          display: none;
          ">
      </video>
      <canvas id="output" style="border:1px solid #000000;"/>
    </div>
  </body>
  <!-- Place your code in the script tag below. You can also use an external .js file -->
  <script>
    const videoWidth = 600;//screen.width;
    const videoHeight = 500;//screen.height;
    var actionCount = 0;
    var lastPair = null;
    var lastActionTime = 0;
    const actionSpanTime = 100;
    const miniDistance = 10;
    var clearCanvas = true;

    function isAndroid() {
      return /Android/i.test(navigator.userAgent);
    }

    function isiOS() {
      return /iPhone|iPad|iPod/i.test(navigator.userAgent);
    }

    function isMobile() {
      return isAndroid() || isiOS();
    }

    async function setupCamera() {
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error(
            'Browser API navigator.mediaDevices.getUserMedia not available');
      }

      const video = document.getElementById('video');
      video.width = videoWidth;
      video.height = videoHeight;

      const mobile = isMobile();
      const stream = await navigator.mediaDevices.getUserMedia({
        'audio': false,
        'video': {
          facingMode: 'user',
          width: mobile ? undefined : videoWidth,
          height: mobile ? undefined : videoHeight,
        },
      });
      video.srcObject = stream;

      return new Promise((resolve) => {
        video.onloadedmetadata = () => {
          resolve(video);
        };
      });
    }

    async function loadVideo() {
      const video = await setupCamera();
      video.play();

      return video;
    }

    const guiState = {
      // algorithm: 'multi-pose',
      algorithm: 'single-pose',
      input: {
        mobileNetArchitecture: isMobile() ? '0.50' : '0.75',
        outputStride: 16,
        imageScaleFactor: 0.5,
      },
      singlePoseDetection: {
        minPoseConfidence: 0.5,
        minPartConfidence: 0.5,
      },
      multiPoseDetection: {
        maxPoseDetections: 5,
        minPoseConfidence: 0.15,
        minPartConfidence: 0.1,
        nmsRadius: 30.0,
      },
      output: {
        showVideo: false,
        showSkeleton: false,
        showPoints: true,
        showBoundingBox: false,
      },
      net: null,
    };

    function detectPoseInRealTime(video, net) {
      const canvas = document.getElementById('output');
      const ctx = canvas.getContext('2d');
      // since images are being fed from a webcam
      const flipHorizontal = true;

      canvas.width = videoWidth;
      canvas.height = videoHeight;

      async function poseDetectionFrame() {
        if (guiState.changeToArchitecture) {
          // Important to purge variables and free up GPU memory
          guiState.net.dispose();

          // Load the PoseNet model weights for either the 0.50, 0.75, 1.00, or 1.01
          // version
          guiState.net = await posenet.load(+guiState.changeToArchitecture);

          guiState.changeToArchitecture = null;
        }

        // Begin monitoring code for frames per second
        // stats.begin();

        // Scale an image down to a certain factor. Too large of an image will slow
        // down the GPU
        const imageScaleFactor = guiState.input.imageScaleFactor;
        const outputStride = +guiState.input.outputStride;

        let poses = [];
        let minPoseConfidence;
        let minPartConfidence;
        switch (guiState.algorithm) {
          case 'single-pose':
            const pose = await guiState.net.estimateSinglePose(
                video, imageScaleFactor, flipHorizontal, outputStride);
            poses.push(pose);

            minPoseConfidence = +guiState.singlePoseDetection.minPoseConfidence;
            minPartConfidence = +guiState.singlePoseDetection.minPartConfidence;
            break;
          case 'multi-pose':
            poses = await guiState.net.estimateMultiplePoses(
                video, imageScaleFactor, flipHorizontal, outputStride,
                guiState.multiPoseDetection.maxPoseDetections,
                guiState.multiPoseDetection.minPartConfidence,
                guiState.multiPoseDetection.nmsRadius);

            minPoseConfidence = +guiState.multiPoseDetection.minPoseConfidence;
            minPartConfidence = +guiState.multiPoseDetection.minPartConfidence;
            break;
        }

        // ctx.clearRect(0, 0, videoWidth, videoHeight);

        if (guiState.output.showVideo) {
          ctx.save();
          ctx.scale(-1, 1);
          ctx.translate(-videoWidth, 0);
          ctx.drawImage(video, 0, 0, videoWidth, videoHeight);
          ctx.restore();
        }

        // For each pose (i.e. person) detected in an image, loop through the poses
        // and draw the resulting skeleton and keypoints if over certain confidence
        // scores
        poses.forEach(({score, keypoints}) => {
          if (score >= minPoseConfidence) {

            const now = new Date().getTime();

            // if(actionCount % 20 == 0){
            //   ctx.clearRect(0, 0, videoWidth, videoHeight);
            //   lastKeypoint = null;
            // }

            if (guiState.output.showPoints) {
              filteredKeypoints = filterKeypoints(["leftWrist", "rightWrist"],
                keypoints, minPartConfidence);
              // console.log(JSON.stringify(filteredKeypoints));
              if(filteredKeypoints.length > 0){
                if(lastPair == null){
                  lastPair = splitKeyPoints(filteredKeypoints);
                  lastActionTime = now;
                }else{
                  if(actionCount >= 2){
                    ctx.clearRect(0, 0, videoWidth, videoHeight);
                    actionCount = 0;
                  }
                  currentPair = splitKeyPoints(filteredKeypoints);
                  // left
                  if(lastPair.left != null && currentPair.left != null &&
                    calcDistance(lastPair.left, currentPair.left) >
                    miniDistance){
                    drawSegment([lastPair.left.position.y,
                      lastPair.left.position.x],
                      [currentPair.left.position.y,
                      currentPair.left.position.x], "black", 1, ctx);

                    lastPair.left = currentPair.left;
                    actionCount++;
                  }

                  //right
                  if(lastPair.right != null && currentPair.right != null &&
                    calcDistance(lastPair.right, currentPair.right) >
                    miniDistance){
                    drawSegment([lastPair.right.position.y,
                      lastPair.right.position.x],
                      [currentPair.right.position.y,
                      currentPair.right.position.x], "yellow", 1, ctx);

                    lastPair.right = currentPair.right;
                    actionCount++;
                  }

                  lastActionTime = now;
                }
                drawKeypoints(filteredKeypoints, minPartConfidence, ctx, scale=1,
                  radius=10);
              }
            }
            if (guiState.output.showSkeleton) {
              drawSkeleton(keypoints, minPartConfidence, ctx);
            }
            if (guiState.output.showBoundingBox) {
              drawBoundingBox(keypoints, ctx);
            }
          }
        });

        // End monitoring code for frames per second
        // stats.end();

        requestAnimationFrame(poseDetectionFrame);
      }

      function calcDistance(point1, point2){
        if(point1.position.x <= 0 || point1.position.x >= 600){
          console.log("x1=" + point1.position.x);
        }
        if(point2.position.x <= 0 || point2.position.x >= 600){
          console.log("x2=" + point2.position.x);
        }
        if(point1.position.y <= 0 || point1.position.y >= 500){
          console.log("y1=" + point1.position.y);
        }
        if(point2.position.y <=0 || point2.position.y >= 500){
          console.log("y2=" + point2.position.y);
        }
        diff = Math.sqrt(Math.pow(point1.position.x - point2.position.x, 2) +
          Math.pow(point1.position.y - point2.position.y, 2));
          // console.log("diff=" + diff)
        return diff;
      }

      function splitKeyPoints(points){
        pair = {left: null, right: null};
        for(i in points){
          point = points[i];
          if(point.part.startsWith("left")){
            pair.left = point;
          }else if(point.part.startsWith("right")){
            pair.right = point;
          }
        }

        return pair;
      }

      function filterKeypoints(part_names, keypoints, minPartConfidence){
        result = [];
        for(point_index in keypoints){
          point = keypoints[point_index]
          for(name_index in part_names){
            if(part_names[name_index] == point.part &&
              point.score > minPartConfidence){
              result.push(point);
            }
          }
        }

        return result;
      }

      poseDetectionFrame();
    }

    // const color = 'red';
    const boundingBoxColor = 'red';
    const lineWidth = 2;

    function toTuple({ y, x }) {
      return [y, x];
    }

    function drawPoint(ctx, y, x, r, color) {
      ctx.beginPath();
      ctx.arc(x, y, r, 0, 2 * Math.PI);
      ctx.fillStyle = color;
      ctx.fill();
    }

    /**
     * Draws a line on a canvas, i.e. a joint
     */
    function drawSegment([ay, ax], [by, bx], color, scale, ctx) {
      ctx.beginPath();
      ctx.moveTo(ax * scale, ay * scale);
      ctx.lineTo(bx * scale, by * scale);
      ctx.lineWidth = lineWidth;
      ctx.strokeStyle = color;
      ctx.stroke();
    }

    /**
     * Draws a pose skeleton by looking up all adjacent keypoints/joints
     */
    function drawSkeleton(keypoints, minConfidence, ctx, scale = 1) {
      const adjacentKeyPoints = posenet.getAdjacentKeyPoints(keypoints, minConfidence);

      adjacentKeyPoints.forEach(keypoints => {
        drawSegment(toTuple(keypoints[0].position), toTuple(keypoints[1].position), color, scale, ctx);
      });
    }

    /**
     * Draw pose keypoints onto a canvas
     */
    function drawKeypoints(keypoints, minConfidence, ctx, scale=1, radius = 3) {
      for (let i = 0; i < keypoints.length; i++) {
        const keypoint = keypoints[i];

        if (keypoint.score < minConfidence) {
          continue;
        }

        const { y, x } = keypoint.position;
        if(keypoint.part.startsWith("left")){
          color = "blue"
        }else if(keypoint.part.startsWith("right")){
          color = "red"
        }else{
          color = "green"
        }
        drawPoint(ctx, y * scale, x * scale, radius, color);
      }
    }

    /**
     * Draw the bounding box of a pose. For example, for a whole person standing
     * in an image, the bounding box will begin at the nose and extend to one of
     * ankles
     */
    function drawBoundingBox(keypoints, ctx) {
      const boundingBox = posenet.getBoundingBox(keypoints);

      ctx.rect(boundingBox.minX, boundingBox.minY, boundingBox.maxX - boundingBox.minX, boundingBox.maxY - boundingBox.minY);

      ctx.strokeStyle = boundingBoxColor;
      ctx.stroke();
    }

    /**
     * Converts an arary of pixel data into an ImageData object
     */
    async function renderToCanvas(a, ctx) {
      const [height, width] = a.shape;
      const imageData = new ImageData(width, height);

      const data = await a.data();

      for (let i = 0; i < height * width; ++i) {
        const j = i * 4;
        const k = i * 3;

        imageData.data[j + 0] = data[k + 0];
        imageData.data[j + 1] = data[k + 1];
        imageData.data[j + 2] = data[k + 2];
        imageData.data[j + 3] = 255;
      }

      ctx.putImageData(imageData, 0, 0);
    }

    /**
     * Draw an image on a canvas
     */
    function renderImageToCanvas(image, size, canvas) {
      canvas.width = size[0];
      canvas.height = size[1];
      const ctx = canvas.getContext('2d');

      ctx.drawImage(image, 0, 0);
    }

    async function bindPage() {

      // Load the PoseNet model weights with architecture 0.75
      const net = await posenet.load(0.75);
      console.log("posenet load completed");
      guiState.net = net;

      document.getElementById('loading').style.display = 'none';
      document.getElementById('main').style.display = 'block';
      document.getElementById('clear').onclick = function(){
        output = document.getElementById('output');
        ctx = output.getContext('2d');
        ctx.clearRect(0, 0, videoWidth, videoHeight);
      };

      let video;

      try {
        video = await loadVideo();
        console.log('load video completed');
      } catch (e) {
        let info = document.getElementById('info');
        info.textContent = 'this browser does not support video capture,' + 'or this device does not have a camera';
        info.style.display = 'block';
        throw e;
      }

      detectPoseInRealTime(video, net);
    }

    navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
    // kick off the demo
    bindPage();
  </script>
</html>
